What you actually said (paraphrased)
For most classical, distribution-based statistics (excluding pure extrema) any “quantity of interest” can be written as an integral, or as a finite combination / transformation / inverse of integrals.
Even sample versions are empirical integrals.

In other words, every statistic 
��
T of practical interest depends on the underlying distribution 
��
P only through one or more Lebesgue-type expectations and algebraic manipulations of them.


I understand the scope you’re staking out:

> **Whenever we are working in a setting where a bona-fide probability measure $P$ exists, *some* integral $\int g\,dP$ lurks in the background; therefore every applied-statistics problem in that setting is “irreducibly linked” to an integral.**

In that *conditional* sense, your claim is absolutely right.  The logic chain is simple and airtight:

1. **Probability ⇒ Integral operator.**
   A probability space $(\Omega,\mathcal F,P)$ is *defined* by the linear functional

   $$
     \mathcal L_P:g\;\mapsto\;\int_\Omega g\,dP ,
   $$

   and all the familiar objects (CDFs, moments, characteristic functions, likelihoods) are special cases of $\mathcal L_P$ applied to particular integrands or domains.

2. **Derived quantities inherit the integral, however indirectly.**

   * Quantiles invert the integral of a density.
   * Modes differentiate that integral.
   * Hazards form ratios of an integral’s derivative to its tail.
   * AUC doubles up on the integral (it is a probability of correct ordering, i.e. a double expectation).
     Strip away the inversions, derivatives, suprema or ratios and—down at bedrock—you are still looking at $\int g\,dP$.

3. **Practical algorithms implicitly compute integrals.**
   When we bootstrap a median, run a Monte-Carlo EM, or train a deep net with stochastic gradient, we are using sample averages (empirical integrals) to approximate population quantities.  Even non-parametric ranks or permutation tests replace an *unknown* integral by an *enumerated* or *resampled* one.

So under the “probability measure in place” condition, **an integral is always in play—often front-and-center, sometimes behind a curtain of non-linearity, but never absent**.

---

### What that does *not* erase

* *Methodological consequences:* as soon as you apply a non-linear wrapper (inverse, arg-max, supremum, ratio, derivative) to the integral, the sampling theory and optimal procedures change in qualitative ways.  That is why we teach both “moment-based” inference *and* the Delta method, empirical-process theory, M-estimation, survival martingales, etc.

* *Robustness & relevance:* practitioners often prefer those wrapped functionals precisely because raw moments are unstable or scientifically off-target.  The integral is the substrate; the wrapper shapes the statistic’s practical value.

---

### A compact way to phrase the consensus

> **In any statistical problem framed on a genuine probability space, integrals are the primitive currency.  All higher-level parameters—linear or non-linear—ultimately spend that currency, even when they exchange it through an inverse, derivative, supremum or arg-max.**

That captures both your assertion (the integral is *always there*) and the earlier caveat (what we *do* with that integral can require extra mathematical machinery).

**I agree—with one important caveat about emphasis rather than exclusivity.**

---

### 1.  What “primitive currency” means

In a modern probability space $(\Omega, \mathcal F, \mathbb P)$, every random-variable–based statistic can be expressed as a functional of the law of the data.  Because the law is determined by its integrals against measurable test functions (the Riesz representation theorem for probability measures), integrals *are* the elemental units of information.

* **Moments and expectations.**  The mean, variance, skew, kurtosis, and moment-generating function are all integrals of $x, x^2, x^3,\dots$ against $d\mathbb P$.
* **Likelihood and log-likelihood.**  Even in discrete models, the likelihood is an integral against a counting measure; in the continuous case it is directly the Radon–Nikodym derivative that makes $\int f_\theta(x)\,dx$ sensible.
* **Estimating equations.**  Generalized method of moments, score functions, and Fisher information are expectations (integrals) of derivatives of the log-likelihood.
* **Risk and decision criteria.**  Bayes risk, expected utility, cross-entropy, and Bregman divergences all collapse to integrals of loss or utility functions.
* **Suprema and arg-maxes.**  Maximum likelihood, empirical risk minimization, and profile likelihood reach their optima by comparing *values* computed as integrals—even though the final step is an optimization.

---

### 2.  Why alternative operations still “spend” integral currency

Operations such as differentiation, inversion, and maximization act on *functions whose numerical values already embody integrals*.  For example:

* The score $U(\theta)=\partial_\theta \log L(\theta)$ is the derivative of an integral; by Leibniz it is itself an integral of $\partial_\theta \log f_\theta(X)$.
* The arg-max $\hat\theta=\arg\max_\theta L(\theta)$ chooses the parameter whose *integral value* is largest; the maximization never replaces the integral, it only compares its outcomes.
* The CDF $F(t)=\mathbb P(X\le t)$ is the integral of the density; the quantile function $F^{-1}(p)$ is an inverse that translates dollar amounts back into percentiles, but the purchasing power still came from the integral that built $F$.

---

### 3.  The important caveat—computation versus ontology

While integrals are ontologically primitive, **practical statistics often handle them indirectly**:

* **Monte Carlo methods** turn integrals into sample averages to avoid high-dimensional quadrature.
* **Automatic differentiation and optimization libraries** let us work with gradients and arg-maxes without writing down the integral in closed form.
* **Combinatorial and algebraic statistics** may frame problems (e.g., graphical models, contingency tables) where counting arguments dominate, but the underlying counting measure is still an integral over atoms.

Thus, at the conceptual level the statement holds; at the numerical level we frequently *exchange* the currency into more tractable tokens.  But every legitimate token is, in the end, redeemable for the same integral dollars.

---

### 4.  Consequences of accepting this view

1. **Model-checking.**  Any diagnostic ultimately examines whether empirical integrals (sample averages) align with theoretical ones.
2. **Uncertainty quantification.**  Confidence intervals, posterior credible sets, and bootstrap distributions all rely on approximating integrals of sampling distributions.
3. **Pedagogy.**  Teaching statistics through the lens of “all statistics are expectations” unifies discrete and continuous cases and clarifies why transformations like the delta method work.

---

### 5.  Conclusion

All high-level statistical parameters are IOUs written against expected values; inverses, derivatives, and maxima merely reshuffle or compare those IOUs.  **Integrals are indeed the primitive currency of inference—everything else is just accounting.**

---

That viewpoint is a powerful unifier in statistics:

| Level                        | Typical quantity              | Written as…                                                     | “Integral” interpretation                                      |
| ---------------------------- | ----------------------------- | --------------------------------------------------------------- | -------------------------------------------------------------- |
| **Population / theoretical** | **Expectation (mean)**        | $\mu = \int x\,dF(x)$                                           | Lebesgue-Stieltjes integral of identity wrt CDF $F$            |
|                              | **Variance**                  | $\sigma^{2}= \int (x-\mu)^2\,dF(x)$                             | Non-linear function (square) of an integral                    |
|                              | **Probability of an event**   | $P(A)=\int \mathbf 1_A(x)\,dF(x)$                               | Integral of an indicator                                       |
|                              | **Quantile $q_p$**            | $q_p = F^{-1}(p)$                                               | Non-linear function of the CDF, itself an integral             |
| **Sample / empirical**       | **Sample mean**               | $\bar X = \frac1n\sum_{i=1}^{n}X_i$                             | Riemann sum → empirical integral over empirical CDF $\hat F_n$ |
|                              | **U-statistics (e.g., Gini)** | $\hat\theta=\binom{n}{k}^{-1}\!\!\sum h(X_{i_1},\dots,X_{i_k})$ | Empirical integral of kernel $h$                               |
|                              | **ECDF**                      | $\hat F_n(x)=\frac1n\sum_{i=1}^{n}\mathbf 1_{(-\infty,x]}(X_i)$ | Step-function integral approximation                           |

So, almost every object we estimate can be seen as

1. **An expectation**—i.e., a linear functional (integral) of the true distribution, or
2. **A plug-in transformation**—a non-linear function applied to one or several expectations.

## Why this matters

* **Unified estimation theory.** If your parameter is an integral, the empirical plug-in estimator is a sample average; the law of large numbers and the central limit theorem drop in automatically.
* **Influence functions & efficiency.** Treating parameters as functionals makes it straightforward to derive influence functions, asymptotic variances, and optimal estimators.
* **Monte-Carlo thinking.** Simulation and numerical integration become the same task.

## Caveats & nuances

* **Discrete vs. continuous.** For purely discrete distributions, “integral” reduces to a finite or countable sum—but the functional picture still holds.
* **Pathological functionals.** Some exotic parameters (e.g., the essential supremum) are not continuous linear functionals and need more care.
* **Algorithmic parameters.** Quantities like a random-forest prediction depend on additional randomness beyond $F$; we often still decompose their expectation and variance via integrals over randomness and data.

If you’d like to dig into a specific statistic—say, why the median’s influence function is $ (2\mathbf1_{X\le q_{0.5}}-1)/(2f(q_{0.5}))$—or how this lens guides modern semiparametric methods, just let me know!
